# ä½œä¸š

- è¡¥å…¨ç¨‹åºä¸­çš„ä»£ç ï¼Œç†è§£å…¶å«ä¹‰ï¼Œå¹¶è·‘é€šæ•´ä¸ªé¡¹ç›®ï¼›
- æŠ¥åå‚åŠ [åƒè¨€æ•°æ®é›†ï¼šä¿¡æ¯æŠ½å–æ¯”èµ›](https://aistudio.baidu.com/aistudio/competition/detail/46)ã€‚

# åŸºäºé¢„è®­ç»ƒæ¨¡å‹å®Œæˆå®ä½“å…³ç³»æŠ½å–

ä¿¡æ¯æŠ½å–æ—¨åœ¨ä»éç»“æ„åŒ–è‡ªç„¶è¯­è¨€æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–çŸ¥è¯†ï¼Œå¦‚å®ä½“ã€å…³ç³»ã€äº‹ä»¶ç­‰ã€‚å¯¹äºç»™å®šçš„è‡ªç„¶è¯­è¨€å¥å­ï¼Œæ ¹æ®é¢„å…ˆå®šä¹‰çš„schemaé›†åˆï¼ŒæŠ½å–å‡ºæ‰€æœ‰æ»¡è¶³schemaçº¦æŸçš„SPOä¸‰å…ƒç»„ã€‚

ä¾‹å¦‚ï¼Œã€Œå¦»å­ã€å…³ç³»çš„schemaå®šä¹‰ä¸ºï¼š      
{      
    S_TYPE: äººç‰©,        
    P: å¦»å­,      
    O_TYPE: {      
        @value: äººç‰©       
    }       
}        

è¯¥ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨PaddleNLPå¿«é€Ÿå®Œæˆå®ä½“å…³ç³»æŠ½å–ï¼Œå‚ä¸[åƒè¨€ä¿¡æ¯æŠ½å–-å…³ç³»æŠ½å–æ¯”èµ›](https://aistudio.baidu.com/aistudio/competition/detail/46)æ‰“æ¦œã€‚





```python
# å®‰è£…paddlenlpæœ€æ–°ç‰ˆæœ¬
!pip install --upgrade paddlenlp

%cd relation_extraction/
```

    Looking in indexes: https://mirror.baidu.com/pypi/simple/
    Collecting paddlenlp
    [?25l  Downloading https://mirror.baidu.com/pypi/packages/b1/e9/128dfc1371db3fc2fa883d8ef27ab6b21e3876e76750a43f58cf3c24e707/paddlenlp-2.0.2-py3-none-any.whl (426kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 430kB 6.8MB/s eta 0:00:01
    [?25hRequirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)
    Requirement already satisfied, skipping upgrade: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.1.1)
    Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)
    Requirement already satisfied, skipping upgrade: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.70.11.1)
    Requirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)
    Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)
    Requirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.9.0)
    Requirement already satisfied, skipping upgrade: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.0.0)
    Requirement already satisfied, skipping upgrade: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.8.53)
    Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.21.0)
    Requirement already satisfied, skipping upgrade: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.8.2)
    Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.22.0)
    Requirement already satisfied, skipping upgrade: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (7.1.2)
    Requirement already satisfied, skipping upgrade: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.7.1.1)
    Requirement already satisfied, skipping upgrade: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.1)
    Requirement already satisfied, skipping upgrade: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.20.3)
    Requirement already satisfied, skipping upgrade: six>=1.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.15.0)
    Requirement already satisfied, skipping upgrade: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.14.0)
    Requirement already satisfied, skipping upgrade: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp) (0.3.3)
    Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)
    Requirement already satisfied, skipping upgrade: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.8.0)
    Requirement already satisfied, skipping upgrade: Jinja2>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.10.1)
    Requirement already satisfied, skipping upgrade: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2019.3)
    Requirement already satisfied, skipping upgrade: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (0.18.0)
    Requirement already satisfied, skipping upgrade: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (3.9.9)
    Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.10.0)
    Requirement already satisfied, skipping upgrade: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.4.10)
    Requirement already satisfied, skipping upgrade: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (2.0.1)
    Requirement already satisfied, skipping upgrade: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (16.7.9)
    Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.0)
    Requirement already satisfied, skipping upgrade: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.4)
    Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < "3.8" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.23)
    Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (5.1.2)
    Requirement already satisfied, skipping upgrade: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.6.0)
    Requirement already satisfied, skipping upgrade: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.6.1)
    Requirement already satisfied, skipping upgrade: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.2.0)
    Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (3.0.4)
    Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2.8)
    Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2019.9.11)
    Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (1.25.6)
    Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (0.16.0)
    Requirement already satisfied, skipping upgrade: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (7.0)
    Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (1.1.0)
    Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)
    Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)
    Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)
    Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.5->Flask-Babel>=1.0.0->visualdl->paddlenlp) (1.1.1)
    Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < "3.8"->pre-commit->visualdl->paddlenlp) (0.6.0)
    Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < "3.8"->pre-commit->visualdl->paddlenlp) (7.2.0)
    Installing collected packages: paddlenlp
      Found existing installation: paddlenlp 2.0.1
        Uninstalling paddlenlp-2.0.1:
          Successfully uninstalled paddlenlp-2.0.1
    Successfully installed paddlenlp-2.0.2
    /home/aistudio/relation_extraction


## å…³ç³»æŠ½å–ä»‹ç»

é’ˆå¯¹ DuIE2.0 ä»»åŠ¡ä¸­å¤šæ¡ã€äº¤å SPOè¿™ä¸€æŠ½å–ç›®æ ‡ï¼Œæ¯”èµ›å¯¹æ ‡å‡†çš„ 'BIO' æ ‡æ³¨è¿›è¡Œäº†æ‰©å±•ã€‚
å¯¹äºæ¯ä¸ª tokenï¼Œæ ¹æ®å…¶åœ¨å®ä½“spanä¸­çš„ä½ç½®ï¼ˆåŒ…æ‹¬Bã€Iã€Oä¸‰ç§ï¼‰ï¼Œæˆ‘ä»¬ä¸ºå…¶æ‰“ä¸Šä¸‰ç±»æ ‡ç­¾ï¼Œå¹¶ä¸”æ ¹æ®å…¶æ‰€å‚ä¸æ„å»ºçš„predicateç§ç±»ï¼Œå°† B æ ‡ç­¾è¿›ä¸€æ­¥åŒºåˆ†ã€‚ç»™å®š schema é›†åˆï¼Œå¯¹äº N ç§ä¸åŒ predicateï¼Œä»¥åŠå¤´å®ä½“/å°¾å®ä½“ä¸¤ç§æƒ…å†µï¼Œæˆ‘ä»¬è®¾è®¡å¯¹åº”çš„å…± 2*N ç§ B æ ‡ç­¾ï¼Œå†åˆå¹¶ I å’Œ O æ ‡ç­¾ï¼Œæ•…æ¯ä¸ª token ä¸€å…±æœ‰ (2*N+2) ä¸ªæ ‡ç­¾ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚


<div align="center">
<img src="https://ai-studio-static-online.cdn.bcebos.com/f984664777b241a9b43ef843c9b752f33906c8916bc146a69f7270b5858bee63" width="500" height="400" alt="æ ‡æ³¨ç­–ç•¥" align=center />
</div>

### è¯„ä»·æ–¹æ³•

å¯¹æµ‹è¯•é›†ä¸Šå‚è¯„ç³»ç»Ÿè¾“å‡ºçš„SPOç»“æœå’Œäººå·¥æ ‡æ³¨çš„SPOç»“æœè¿›è¡Œç²¾å‡†åŒ¹é…ï¼Œé‡‡ç”¨F1å€¼ä½œä¸ºè¯„ä»·æŒ‡æ ‡ã€‚æ³¨æ„ï¼Œå¯¹äºå¤æ‚Oå€¼ç±»å‹çš„SPOï¼Œå¿…é¡»æ‰€æœ‰æ§½ä½éƒ½ç²¾ç¡®åŒ¹é…æ‰è®¤ä¸ºè¯¥SPOæŠ½å–æ­£ç¡®ã€‚é’ˆå¯¹éƒ¨åˆ†æ–‡æœ¬ä¸­å­˜åœ¨å®ä½“åˆ«åçš„é—®é¢˜ï¼Œä½¿ç”¨ç™¾åº¦çŸ¥è¯†å›¾è°±çš„åˆ«åè¯å…¸æ¥è¾…åŠ©è¯„æµ‹ã€‚F1å€¼çš„è®¡ç®—æ–¹å¼å¦‚ä¸‹ï¼š

F1 = (2 * P * R) / (P + R)ï¼Œå…¶ä¸­

- P = æµ‹è¯•é›†æ‰€æœ‰å¥å­ä¸­é¢„æµ‹æ­£ç¡®çš„SPOä¸ªæ•° / æµ‹è¯•é›†æ‰€æœ‰å¥å­ä¸­é¢„æµ‹å‡ºçš„SPOä¸ªæ•°
- R = æµ‹è¯•é›†æ‰€æœ‰å¥å­ä¸­é¢„æµ‹æ­£ç¡®çš„SPOä¸ªæ•° / æµ‹è¯•é›†æ‰€æœ‰å¥å­ä¸­äººå·¥æ ‡æ³¨çš„SPOä¸ªæ•°

### Step1ï¼šæ„å»ºæ¨¡å‹

è¯¥ä»»åŠ¡å¯ä»¥çœ‹ä½œä¸€ä¸ªåºåˆ—æ ‡æ³¨ä»»åŠ¡ï¼Œæ‰€ä»¥åŸºçº¿æ¨¡å‹é‡‡ç”¨çš„æ˜¯ERNIEåºåˆ—æ ‡æ³¨æ¨¡å‹ã€‚

**PaddleNLPæä¾›äº†ERNIEé¢„è®­ç»ƒæ¨¡å‹å¸¸ç”¨åºåˆ—æ ‡æ³¨æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡æŒ‡å®šæ¨¡å‹åå­—å®Œæˆä¸€é”®åŠ è½½ã€‚PaddleNLPä¸ºäº†æ–¹ä¾¿ç”¨æˆ·å¤„ç†æ•°æ®ï¼Œå†…ç½®äº†å¯¹äºå„ä¸ªé¢„è®­ç»ƒæ¨¡å‹å¯¹åº”çš„Tokenizerï¼Œå¯ä»¥å®Œæˆæ–‡æœ¬tokenåŒ–ï¼Œè½¬token IDï¼Œæ–‡æœ¬é•¿åº¦æˆªæ–­ç­‰æ“ä½œã€‚**

æ–‡æœ¬æ•°æ®å¤„ç†ç›´æ¥è°ƒç”¨tokenizerå³å¯è¾“å‡ºæ¨¡å‹æ‰€éœ€è¾“å…¥æ•°æ®ã€‚




```python
import os
import json
from paddlenlp.transformers import ErnieForTokenClassification, ErnieTokenizer

label_map_path = os.path.join('data', "predicate2id.json")

if not (os.path.exists(label_map_path) and os.path.isfile(label_map_path)):
    sys.exit("{} dose not exists or is not a file.".format(label_map_path))
with open(label_map_path, 'r', encoding='utf8') as fp:
    label_map = json.load(fp)
    
num_classes = (len(label_map.keys()) - 2) * 2 + 2

# è¡¥é½ä»£ç ï¼Œç†è§£TokenClassificationæ¥å£å«ä¹‰ï¼Œç†è§£å…³ç³»æŠ½å–æ ‡æ³¨ä½“ç³»å’Œç±»åˆ«æ•°ç”±æ¥
model = ErnieForTokenClassification.from_pretrained("ernie-1.0", num_classes=num_classes)
tokenizer = ErnieTokenizer.from_pretrained("ernie-1.0")

inputs = tokenizer(text="å´å®—å®ªé­æœåŠ¡ç”Ÿç§æ—æ­§è§†, ä»–æ°”å‘›: æˆ‘ä¹°ä¸‹ç¾å›½éƒ½è¡Œ!è‰ºäººç‹„èºä¸å­™é¹18å²çš„ç‹¬å­å­™å®‰ä½èµ´ç¾å›½è¯»é«˜ä¸­", max_seq_len=20)
inputs
```

    [2021-06-13 16:17:20,321] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/ernie/ernie_v1_chn_base.pdparams and saved to /home/aistudio/.paddlenlp/models/ernie-1.0
    [2021-06-13 16:17:20,326] [    INFO] - Downloading ernie_v1_chn_base.pdparams from https://paddlenlp.bj.bcebos.com/models/transformers/ernie/ernie_v1_chn_base.pdparams
    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 392507/392507 [00:08<00:00, 45848.02it/s]
    /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.
      warnings.warn(("Skip loading for {}. ".format(key) + str(err)))
    /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.
      warnings.warn(("Skip loading for {}. ".format(key) + str(err)))
    [2021-06-13 16:17:36,250] [    INFO] - Downloading vocab.txt from https://paddlenlp.bj.bcebos.com/models/transformers/ernie/vocab.txt
    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:00<00:00, 3978.12it/s]





    {'input_ids': [1,
      1167,
      761,
      2075,
      1396,
      231,
      112,
      21,
      106,
      495,
      2752,
      367,
      30,
      44,
      266,
      5706,
      12049,
      75,
      1042,
      2],
     'token_type_ids': [0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0]}



### Step2ï¼šåŠ è½½å¹¶å¤„ç†æ•°æ®


ä»æ¯”èµ›å®˜ç½‘ä¸‹è½½æ•°æ®é›†ï¼Œè§£å‹å­˜æ”¾äºdata/ç›®å½•ä¸‹å¹¶é‡å‘½åä¸ºtrain_data.json, dev_data.json, test_data.json.

æˆ‘ä»¬å¯ä»¥åŠ è½½è‡ªå®šä¹‰æ•°æ®é›†ã€‚é€šè¿‡ç»§æ‰¿[`paddle.io.Dataset`](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/Dataset_cn.html#dataset)ï¼Œè‡ªå®šä¹‰å®ç°`__getitem__` å’Œ `__len__`ä¸¤ä¸ªæ–¹æ³•ã€‚



```python
from typing import Optional, List, Union, Dict

import numpy as np
import paddle
from tqdm import tqdm
from paddlenlp.utils.log import logger

from data_loader import parse_label, DataCollator, convert_example_to_feature
from extract_chinese_and_punct import ChineseAndPunctuationExtractor


class DuIEDataset(paddle.io.Dataset):
    """
    Dataset of DuIE.
    """

    def __init__(
            self,
            input_ids: List[Union[List[int], np.ndarray]],
            seq_lens: List[Union[List[int], np.ndarray]],
            tok_to_orig_start_index: List[Union[List[int], np.ndarray]],
            tok_to_orig_end_index: List[Union[List[int], np.ndarray]],
            labels: List[Union[List[int], np.ndarray, List[str], List[Dict]]]):
        super(DuIEDataset, self).__init__()

        self.input_ids = input_ids
        self.seq_lens = seq_lens
        self.tok_to_orig_start_index = tok_to_orig_start_index
        self.tok_to_orig_end_index = tok_to_orig_end_index
        self.labels = labels

    def __len__(self):
        if isinstance(self.input_ids, np.ndarray):
            return self.input_ids.shape[0]
        else:
            return len(self.input_ids)

    def __getitem__(self, item):
        return {
            "input_ids": np.array(self.input_ids[item]),
            "seq_lens": np.array(self.seq_lens[item]),
            "tok_to_orig_start_index":
            np.array(self.tok_to_orig_start_index[item]),
            "tok_to_orig_end_index": np.array(self.tok_to_orig_end_index[item]),
            # If model inputs is generated in `collate_fn`, delete the data type casting.
            "labels": np.array(
                self.labels[item], dtype=np.float32),
        }

    @classmethod
    def from_file(cls,
                  file_path: Union[str, os.PathLike],
                  tokenizer: ErnieTokenizer,
                  max_length: Optional[int]=512,
                  pad_to_max_length: Optional[bool]=None):
        assert os.path.exists(file_path) and os.path.isfile(
            file_path), f"{file_path} dose not exists or is not a file."
        label_map_path = os.path.join(
            os.path.dirname(file_path), "predicate2id.json")
        assert os.path.exists(label_map_path) and os.path.isfile(
            label_map_path
        ), f"{label_map_path} dose not exists or is not a file."
        with open(label_map_path, 'r', encoding='utf8') as fp:
            label_map = json.load(fp)
        chineseandpunctuationextractor = ChineseAndPunctuationExtractor()

        input_ids, seq_lens, tok_to_orig_start_index, tok_to_orig_end_index, labels = (
            [] for _ in range(5))
        dataset_scale = sum(1 for line in open(file_path, 'r'))
        logger.info("Preprocessing data, loaded from %s" % file_path)
        with open(file_path, "r", encoding="utf-8") as fp:
            lines = fp.readlines()
            for line in tqdm(lines):
                example = json.loads(line)
                input_feature = convert_example_to_feature(
                    example, tokenizer, chineseandpunctuationextractor,
                    label_map, max_length, pad_to_max_length)
                input_ids.append(input_feature.input_ids)
                seq_lens.append(input_feature.seq_len)
                tok_to_orig_start_index.append(
                    input_feature.tok_to_orig_start_index)
                tok_to_orig_end_index.append(
                    input_feature.tok_to_orig_end_index)
                labels.append(input_feature.labels)

        return cls(input_ids, seq_lens, tok_to_orig_start_index,
                   tok_to_orig_end_index, labels)

```


```python
data_path = 'data'
batch_size = 32
max_seq_length = 128

train_file_path = os.path.join(data_path, 'train_data.json')
train_dataset = DuIEDataset.from_file(
    train_file_path, tokenizer, max_seq_length, True)
train_batch_sampler = paddle.io.BatchSampler(
    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)
collator = DataCollator()
train_data_loader = paddle.io.DataLoader(
    dataset=train_dataset,
    batch_sampler=train_batch_sampler,
    collate_fn=collator)

eval_file_path = os.path.join(data_path, 'dev_data.json')
test_dataset = DuIEDataset.from_file(
    eval_file_path, tokenizer, max_seq_length, True)
test_batch_sampler = paddle.io.BatchSampler(
    test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)
test_data_loader = paddle.io.DataLoader(
    dataset=test_dataset,
    batch_sampler=test_batch_sampler,
    collate_fn=collator)
```

    [2021-06-13 16:18:27,147] [    INFO] - Preprocessing data, loaded from data/train_data.json
    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10010/10010 [00:18<00:00, 553.95it/s]
    [2021-06-13 16:18:45,273] [    INFO] - Preprocessing data, loaded from data/dev_data.json
    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:01<00:00, 569.48it/s]


### Step3ï¼šå®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ï¼Œå¼€å§‹è®­ç»ƒ

æˆ‘ä»¬é€‰æ‹©å‡æ–¹è¯¯å·®ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œä½¿ç”¨[`paddle.optimizer.AdamW`](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/optimizer/adamw/AdamW_cn.html#adamw)ä½œä¸ºä¼˜åŒ–å™¨ã€‚



åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä¿å­˜åœ¨å½“å‰ç›®å½•checkpointsæ–‡ä»¶å¤¹ä¸‹ã€‚åŒæ—¶åœ¨è®­ç»ƒçš„åŒæ—¶ä½¿ç”¨å®˜æ–¹è¯„æµ‹è„šæœ¬è¿›è¡Œè¯„ä¼°ï¼Œè¾“å‡ºP/R/F1æŒ‡æ ‡ã€‚
åœ¨éªŒè¯é›†ä¸ŠF1å¯ä»¥è¾¾åˆ°69.42ã€‚



```python
import paddle.nn as nn

class BCELossForDuIE(nn.Layer):
    def __init__(self, ):
        super(BCELossForDuIE, self).__init__()
        self.criterion = nn.BCEWithLogitsLoss(reduction='none')

    def forward(self, logits, labels, mask):
        loss = self.criterion(logits, labels)
        mask = paddle.cast(mask, 'float32')
        loss = loss * mask.unsqueeze(-1)
        loss = paddle.sum(loss.mean(axis=2), axis=1) / paddle.sum(mask, axis=1)
        loss = loss.mean()
        return loss
```


```python
from utils import write_prediction_results, get_precision_recall_f1, decoding

@paddle.no_grad()
def evaluate(model, criterion, data_loader, file_path, mode):
    """
    mode eval:
    eval on development set and compute P/R/F1, called between training.
    mode predict:
    eval on development / test set, then write predictions to \
        predict_test.json and predict_test.json.zip \
        under /home/aistudio/relation_extraction/data dir for later submission or evaluation.
    """
    example_all = []
    with open(file_path, "r", encoding="utf-8") as fp:
        for line in fp:
            example_all.append(json.loads(line))
    id2spo_path = os.path.join(os.path.dirname(file_path), "id2spo.json")
    with open(id2spo_path, 'r', encoding='utf8') as fp:
        id2spo = json.load(fp)

    model.eval()
    loss_all = 0
    eval_steps = 0
    formatted_outputs = []
    current_idx = 0
    for batch in tqdm(data_loader, total=len(data_loader)):
        eval_steps += 1
        input_ids, seq_len, tok_to_orig_start_index, tok_to_orig_end_index, labels = batch
        logits = model(input_ids=input_ids)
        mask = (input_ids != 0).logical_and((input_ids != 1)).logical_and((input_ids != 2))
        loss = criterion(logits, labels, mask)
        loss_all += loss.numpy().item()
        probs = F.sigmoid(logits)
        logits_batch = probs.numpy()
        seq_len_batch = seq_len.numpy()
        tok_to_orig_start_index_batch = tok_to_orig_start_index.numpy()
        tok_to_orig_end_index_batch = tok_to_orig_end_index.numpy()
        formatted_outputs.extend(decoding(example_all[current_idx: current_idx+len(logits)],
                                          id2spo,
                                          logits_batch,
                                          seq_len_batch,
                                          tok_to_orig_start_index_batch,
                                          tok_to_orig_end_index_batch))
        current_idx = current_idx+len(logits)
    loss_avg = loss_all / eval_steps
    print("eval loss: %f" % (loss_avg))

    if mode == "predict":
        predict_file_path = os.path.join("/home/aistudio/relation_extraction/data", 'predictions.json')
    else:
        predict_file_path = os.path.join("/home/aistudio/relation_extraction/data", 'predict_eval.json')

    predict_zipfile_path = write_prediction_results(formatted_outputs,
                                                    predict_file_path)

    if mode == "eval":
        precision, recall, f1 = get_precision_recall_f1(file_path,
                                                        predict_zipfile_path)
        # os.system('rm {} {}'.format(predict_file_path, predict_zipfile_path))
        return precision, recall, f1
    elif mode != "predict":
        raise Exception("wrong mode for eval func")
```


```python
from paddlenlp.transformers import LinearDecayWithWarmup

learning_rate = 2e-5
num_train_epochs = 5
warmup_ratio = 0.06

criterion = BCELossForDuIE()
# Defines learning rate strategy.
steps_by_epoch = len(train_data_loader)
num_training_steps = steps_by_epoch * num_train_epochs
lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_ratio)
optimizer = paddle.optimizer.AdamW(
    learning_rate=lr_scheduler,
    parameters=model.parameters(),
    apply_decay_param_fun=lambda x: x in [
        p.name for n, p in model.named_parameters()
        if not any(nd in n for nd in ["bias", "norm"])])
```


```python
# æ¨¡å‹å‚æ•°ä¿å­˜è·¯å¾„
!mkdir checkpoints
```

### Step4ï¼šæäº¤é¢„æµ‹ç»“æœ

åŠ è½½è®­ç»ƒä¿å­˜çš„æ¨¡å‹åŠ è½½åè¿›è¡Œé¢„æµ‹ã€‚

**NOTE:** æ³¨æ„è®¾ç½®ç”¨äºé¢„æµ‹çš„æ¨¡å‹å‚æ•°è·¯å¾„ã€‚


```python
import time
import paddle.nn.functional as F

# Starts training.
global_step = 0
logging_steps = 50
save_steps = 10000
num_train_epochs = 2
output_dir = 'checkpoints'
tic_train = time.time()
model.train()
for epoch in range(num_train_epochs):
    print("\n=====start training of %d epochs=====" % epoch)
    tic_epoch = time.time()
    for step, batch in enumerate(train_data_loader):
        input_ids, seq_lens, tok_to_orig_start_index, tok_to_orig_end_index, labels = batch
        logits = model(input_ids=input_ids)
        mask = (input_ids != 0).logical_and((input_ids != 1)).logical_and(
            (input_ids != 2))
        loss = criterion(logits, labels, mask)
        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        optimizer.clear_gradients()
        loss_item = loss.numpy().item()

        if global_step % logging_steps == 0:
            print(
                "epoch: %d / %d, steps: %d / %d, loss: %f, speed: %.2f step/s"
                % (epoch, num_train_epochs, step, steps_by_epoch,
                    loss_item, logging_steps / (time.time() - tic_train)))
            tic_train = time.time()

        if global_step % save_steps == 0 and global_step != 0:
            print("\n=====start evaluating ckpt of %d steps=====" %
                    global_step)
            precision, recall, f1 = evaluate(
                model, criterion, test_data_loader, eval_file_path, "eval")
            print("precision: %.2f\t recall: %.2f\t f1: %.2f\t" %
                    (100 * precision, 100 * recall, 100 * f1))
            print("saving checkpoing model_%d.pdparams to %s " %
                    (global_step, output_dir))
            paddle.save(model.state_dict(),
                        os.path.join(output_dir, 
                                        "model_%d.pdparams" % global_step))
            model.train()

        global_step += 1
    tic_epoch = time.time() - tic_epoch
    print("epoch time footprint: %d hour %d min %d sec" %
            (tic_epoch // 3600, (tic_epoch % 3600) // 60, tic_epoch % 60))

# Does final evaluation.
print("\n=====start evaluating last ckpt of %d steps=====" %
        global_step)
precision, recall, f1 = evaluate(model, criterion, test_data_loader,
                                    eval_file_path, "eval")
print("precision: %.2f\t recall: %.2f\t f1: %.2f\t" %
        (100 * precision, 100 * recall, 100 * f1))
paddle.save(model.state_dict(),
            os.path.join(output_dir,
                            "model_%d.pdparams" % global_step))
print("\n=====training complete=====")
```

    
    =====start training of 0 epochs=====
    epoch: 0 / 2, steps: 0 / 312, loss: 0.031310, speed: 193.10 step/s
    epoch: 0 / 2, steps: 50 / 312, loss: 0.028091, speed: 4.28 step/s
    epoch: 0 / 2, steps: 100 / 312, loss: 0.027707, speed: 4.35 step/s
    epoch: 0 / 2, steps: 150 / 312, loss: 0.023921, speed: 4.34 step/s
    epoch: 0 / 2, steps: 200 / 312, loss: 0.023447, speed: 4.35 step/s
    epoch: 0 / 2, steps: 250 / 312, loss: 0.024026, speed: 4.23 step/s
    epoch: 0 / 2, steps: 300 / 312, loss: 0.022012, speed: 4.34 step/s
    epoch time footprint: 0 hour 1 min 12 sec
    
    =====start training of 1 epochs=====
    epoch: 1 / 2, steps: 38 / 312, loss: 0.021460, speed: 4.31 step/s
    epoch: 1 / 2, steps: 88 / 312, loss: 0.023060, speed: 4.33 step/s
    epoch: 1 / 2, steps: 138 / 312, loss: 0.020697, speed: 4.34 step/s
    epoch: 1 / 2, steps: 188 / 312, loss: 0.019150, speed: 4.23 step/s
    epoch: 1 / 2, steps: 238 / 312, loss: 0.019095, speed: 4.31 step/s
    epoch: 1 / 2, steps: 288 / 312, loss: 0.016953, speed: 4.34 step/s
    epoch time footprint: 0 hour 1 min 12 sec
    
    =====start evaluating last ckpt of 624 steps=====


    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:02<00:00, 11.09it/s]


    eval loss: 0.016831
    precision: 0.00	 recall: 0.00	 f1: 0.00	
    
    =====training complete=====



```python
!bash predict.sh
```

    + export CUDA_VISIBLE_DEVICES=0
    + CUDA_VISIBLE_DEVICES=0
    + export BATCH_SIZE=8
    + BATCH_SIZE=8
    + export CKPT=./checkpoints/model_624.pdparams
    + CKPT=./checkpoints/model_624.pdparams
    + export DATASET_FILE=./data/test_data.json
    + DATASET_FILE=./data/test_data.json
    + python run_duie.py --do_predict --init_checkpoint ./checkpoints/model_624.pdparams --predict_data_file ./data/test_data.json --max_seq_length 512 --batch_size 8
    [2021-06-13 16:36:37,845] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-1.0/ernie_v1_chn_base.pdparams
    W0613 16:36:37.847292  2019 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1
    W0613 16:36:37.852174  2019 device_context.cc:422] device: 0, cuDNN Version: 7.6.
    /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.
      warnings.warn(("Skip loading for {}. ".format(key) + str(err)))
    /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.
      warnings.warn(("Skip loading for {}. ".format(key) + str(err)))
    [2021-06-13 16:36:44,557] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-1.0/vocab.txt
    [2021-06-13 16:36:44,574] [    INFO] - Preprocessing data, loaded from ./data/test_data.json
    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:04<00:00, 231.44it/s]
    
    =====start predicting=====
    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:12<00:00, 10.40it/s]
    eval loss: 0.029999
    =====predicting complete=====


é¢„æµ‹ç»“æœä¼šè¢«ä¿å­˜åœ¨data/predictions.jsonï¼Œdata/predictions.json.zipï¼Œå…¶æ ¼å¼ä¸åŸæ•°æ®é›†æ–‡ä»¶ä¸€è‡´ã€‚

ä¹‹åå¯ä»¥ä½¿ç”¨å®˜æ–¹è¯„ä¼°è„šæœ¬è¯„ä¼°è®­ç»ƒæ¨¡å‹åœ¨dev_data.jsonä¸Šçš„æ•ˆæœã€‚å¦‚ï¼š

```shell
python re_official_evaluation.py --golden_file=dev_data.json  --predict_file=predicitons.json.zip [--alias_file alias_dict]
```
è¾“å‡ºæŒ‡æ ‡ä¸ºPrecision, Recall å’Œ F1ï¼ŒAlias fileåŒ…å«äº†åˆæ³•çš„å®ä½“åˆ«åï¼Œæœ€ç»ˆè¯„æµ‹çš„æ—¶å€™ä¼šä½¿ç”¨ï¼Œè¿™é‡Œä¸äºˆæä¾›ã€‚

ä¹‹ååœ¨test_data.jsonä¸Šé¢„æµ‹ï¼Œç„¶åé¢„æµ‹ç»“æœï¼ˆ.zipæ–‡ä»¶ï¼‰è‡³[åƒè¨€è¯„æµ‹é¡µé¢](https://aistudio.baidu.com/aistudio/competition/detail/46)ã€‚





![](https://ai-studio-static-online.cdn.bcebos.com/16e3f941dcfb403396ac6dd7cecb746068e7410a7a8040f989a0fa7c305e4049)



## Tricks

### å°è¯•æ›´å¤šçš„é¢„è®­ç»ƒæ¨¡å‹

åŸºçº¿é‡‡ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹ä¸ºERNIEï¼ŒPaddleNLPæä¾›äº†ä¸°å¯Œçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¦‚BERTï¼ŒRoBERTaï¼ŒElectraï¼ŒXLNetç­‰
å‚è€ƒ[é¢„è®­ç»ƒæ¨¡å‹æ–‡æ¡£](https://paddlenlp.readthedocs.io/zh/latest/model_zoo/transformers.html)

å¦‚å¯ä»¥é€‰æ‹©RoBERTa largeä¸­æ–‡æ¨¡å‹ä¼˜åŒ–æ¨¡å‹æ•ˆæœï¼Œåªéœ€æ›´æ¢æ¨¡å‹å’Œtokenizerå³å¯æ— ç¼è¡”æ¥ã€‚


```python
from paddlenlp.transformers import RobertaForTokenClassification, RobertaTokenizer

model = RobertaForTokenClassification.from_pretrained(
    "roberta-wwm-ext-large",
    num_classes=(len(label_map) - 2) * 2 + 2)
tokenizer = RobertaTokenizer.from_pretrained("roberta-wwm-ext-large")
```

### æ¨¡å‹é›†æˆ

ä½¿ç”¨å¤šä¸ªæ¨¡å‹è¿›è¡Œè®­ç»ƒé¢„æµ‹ï¼Œå°†å„ä¸ªæ¨¡å‹é¢„æµ‹ç»“æœè¿›è¡Œèåˆã€‚

ä»¥ä¸ŠåŸºçº¿å®ç°åŸºäºPaddleNLPï¼Œå¼€æºä¸æ˜“ï¼Œå¸Œæœ›å¤§å®¶å¤šå¤šæ”¯æŒ~ 
**è®°å¾—ç»™[PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)ç‚¹ä¸ªå°å°çš„Starâ­ï¼ŒåŠæ—¶è·Ÿè¸ªæœ€æ–°æ¶ˆæ¯å’ŒåŠŸèƒ½å“¦**

GitHubåœ°å€ï¼š[https://github.com/PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)

